{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "import models\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models: MobileNet, ResNet18 32 64\n",
    "# modes: mixup, instahide\n",
    "\n",
    "args = {\n",
    "    'model': 'ResNet18',\n",
    "    'data': 'cifar10',\n",
    "    'nclass': 10,\n",
    "    'lr': 0.01,\n",
    "    'batch_size': 128,\n",
    "    'epoch': 100,\n",
    "    'augment': True,\n",
    "    'decay': 1e-4,\n",
    "    'name': 'cross',\n",
    "    'seed': 0,\n",
    "    'resume': False,\n",
    "    'klam': 4,\n",
    "    'mode': 'perturb',\n",
    "    'upper': 0.65,\n",
    "    'trial': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def label_to_onehot(target, num_classes=args['nclass']):\n",
    "    '''Returns one-hot embeddings of scaler labels'''\n",
    "    target = torch.unsqueeze(target, 1)\n",
    "    onehot_target = torch.zeros(target.size(\n",
    "        0), num_classes, device=target.device)\n",
    "    onehot_target.scatter_(1, target, 1)\n",
    "    return onehot_target\n",
    "\n",
    "\n",
    "def cross_entropy_for_onehot(pred, target):\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))\n",
    "\n",
    "\n",
    "def mixup_criterion(pred, ys, lam_batch, num_class=args['nclass']):\n",
    "    '''Returns mixup loss'''\n",
    "    ys_onehot = [label_to_onehot(y, num_classes=num_class) for y in ys]\n",
    "    mixy = vec_mul_ten(lam_batch[:, 0], ys_onehot[0])\n",
    "    for i in range(1, args['klam']):\n",
    "        mixy += vec_mul_ten(lam_batch[:, i], ys_onehot[i])\n",
    "    l = cross_entropy_for_onehot(pred, mixy)\n",
    "    return l\n",
    "\n",
    "\n",
    "def vec_mul_ten(vec, tensor):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    size = list(tensor.size())\n",
    "    size[0] = -1\n",
    "    size_rs = [1 for i in range(len(size))]\n",
    "    size_rs[0] = -1\n",
    "    vec = vec.reshape(size_rs).expand(size)\n",
    "    res = vec * tensor\n",
    "    return res\n",
    "\n",
    "\n",
    "def mixup_data(x, y, use_cuda=True, perturbed_examples=None, perturb_labl=None):\n",
    "    '''Returns mixed inputs, lists of targets, and lambdas'''\n",
    "    lams = np.random.normal(0, 1, size=(x.size()[0], args['klam']))\n",
    "    for i in range(x.size()[0]):\n",
    "        lams[i] = np.abs(lams[i]) / np.sum(np.abs(lams[i]))\n",
    "        if args['klam'] > 1:\n",
    "            while lams[i].max() > args['upper']:     # upper bounds a single lambda (or (lams[i][0] + lams[i][1]) < args['dom'])\n",
    "                lams[i] = np.random.normal(0, 1, size=(1, args['klam']))\n",
    "                lams[i] = np.abs(lams[i]) / np.sum(np.abs(lams[i]))\n",
    "\n",
    "    lams = torch.from_numpy(lams).float().to(device)\n",
    "\n",
    "    mixed_x = vec_mul_ten(lams[:, 0], x)\n",
    "    ys = [y]\n",
    "    \n",
    "    if args['mode'] == 'perturb':\n",
    "        batch_size = perturbed_examples.size()[0]\n",
    "        index = torch.randperm(batch_size).to(device)\n",
    "        mixed_x += vec_mul_ten(lams[:, 1], perturbed_examples[index, :])\n",
    "        ys.append(perturb_labl[index])\n",
    "        for i in range(1, args['klam']):\n",
    "            batch_size = x.size()[0]\n",
    "            index = torch.randperm(batch_size).to(device)\n",
    "            mixed_x  += vec_mul_ten(lams[:, i], x[index, :])\n",
    "            ys.append(y[index])\n",
    "        \n",
    "    else:\n",
    "        for i in range(1, args['klam']):\n",
    "            batch_size = x.size()[0]\n",
    "            index = torch.randperm(batch_size).to(device)\n",
    "            mixed_x  += vec_mul_ten(lams[:, i], x[index, :])\n",
    "            ys.append(y[index])         # Only keep the labels for private samples\n",
    "\n",
    "    if args['mode'] == 'instahide': # TODO -> from adding random flip mask, \n",
    "        sign = torch.randint(2, size=list(x.shape), device=device) * 2.0 - 1\n",
    "        mixed_x *= sign.float().to(device)\n",
    "    return mixed_x, ys, lams\n",
    "\n",
    "\n",
    "def generate_sample(trainloader):\n",
    "    assert len(trainloader) == 1        # Load all training data once\n",
    "    \n",
    "    if args['mode'] == 'perturb':\n",
    "        perturbed_examples = np.load('results/perturbed.npy')\n",
    "        perturbed_lables = np.load('results/perturbed_y.npy')\n",
    "\n",
    "    for _, (inputs, targets) in enumerate(trainloader):\n",
    "        if use_cuda:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if args['mode'] == 'perturb':\n",
    "                perturbed_examples, perturbed_lables = torch.Tensor(perturbed_examples).to(device), torch.Tensor(perturbed_lables).to(device)\n",
    "        if args['mode'] == 'perturb':\n",
    "            mix_inputs, mix_targets, lams = mixup_data(\n",
    "                inputs, targets.float(), use_cuda, perturbed_examples, perturbed_lables)\n",
    "        else:\n",
    "            mix_inputs, mix_targets, lams = mixup_data(\n",
    "                inputs, targets.float(), use_cuda)\n",
    "    return (mix_inputs, mix_targets, inputs, targets, lams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, inputs_all, mix_targets_all, lams, epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    seq = random.sample(range(len(inputs_all)), len(inputs_all))\n",
    "    bl = list(chunks(seq, args['batch_size']))\n",
    "\n",
    "    for batch_idx in tqdm(range(len(bl))):\n",
    "        b = bl[batch_idx]\n",
    "        inputs = torch.stack([inputs_all[i] for i in b])\n",
    "        if args['mode'] == 'instahide' or args['mode'] == 'mixup' or args['mode'] == 'perturb':\n",
    "            lam_batch = torch.stack([lams[i] for i in b])\n",
    "\n",
    "        mix_targets = []\n",
    "        for ik in range(args['klam']):\n",
    "            mix_targets.append(\n",
    "                torch.stack(\n",
    "                    [mix_targets_all[ik][ib].long().to(device) for ib in b]))\n",
    "        targets_var = [Variable(mix_targets[ik]) for ik in range(args['klam'])]\n",
    "\n",
    "        inputs = Variable(inputs)\n",
    "        outputs = net(inputs)\n",
    "        loss = mixup_criterion(outputs, targets_var, lam_batch)\n",
    "        train_loss += loss.data.item()\n",
    "        total += args['batch_size']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         progress_bar(batch_idx, len(inputs_all)/args['batch_size']+1,\n",
    "#                      'Loss: %.3f' % (train_loss / (batch_idx + 1)))\n",
    "        \n",
    "#         print(len(inputs_all)/args['batch_size']+1, 'Loss: %.3f' % (train_loss / (batch_idx + 1)))\n",
    "    return (train_loss / batch_idx, 100. * correct / total)\n",
    "\n",
    "\n",
    "def test(net, optimizer, testloader, epoch, start_epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss, correct_1, correct_5, total = 0, 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(testloader)):\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.data.item()\n",
    "            _, pred = outputs.topk(5, 1, largest=True, sorted=True)\n",
    "            total += targets.size(0)\n",
    "            correct = pred.eq(targets.view(targets.size(0), -\n",
    "                                           1).expand_as(pred)).float().cpu()\n",
    "            correct_1 += correct[:, :1].sum()\n",
    "            correct_5 += correct[:, :5].sum()\n",
    "            \n",
    "#             print('Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "#                 (test_loss /\n",
    "#                     (batch_idx + 1), 100. * correct_1 / total, correct_1, total))\n",
    "\n",
    "#             progress_bar(\n",
    "#                 batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "#                 (test_loss /\n",
    "#                     (batch_idx + 1), 100. * correct_1 / total, correct_1, total))\n",
    "\n",
    "    acc = 100. * correct_1 / total\n",
    "    if epoch == start_epoch + args['epoch'] - 1 or acc > best_acc:\n",
    "        save_checkpoint(net, acc, epoch)\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "    return (test_loss / batch_idx, 100. * correct_1 / total)\n",
    "\n",
    "\n",
    "def save_checkpoint(net, acc, epoch):\n",
    "    \"\"\" Save checkpoints. \"\"\"\n",
    "    print('Saving..')\n",
    "    state = {\n",
    "        'net': net.cpu(),\n",
    "        'acc': acc,\n",
    "        'epoch': epoch,\n",
    "        'rng_state': torch.get_rng_state()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    ckptname = os.path.join(\n",
    "        './checkpoint/', f'{args[\"model\"]}_{args[\"data\"]}_{args[\"mode\"]}_{args[\"klam\"]}_{args[\"name\"]}_0.t7')\n",
    "    torch.save(state, ckptname)\n",
    "    \n",
    "    \n",
    "\n",
    "def prepare_data():\n",
    "    ## --------------- Prepare data --------------- ##\n",
    "    print('==> Preparing data..')\n",
    "\n",
    "    cifar_normalize = transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    \n",
    "    mnist_normalize = transforms.Normalize((0.1307,), (0.3081,))\n",
    "\n",
    "    transform_imagenet = transforms.Compose([\n",
    "        transforms.Resize(40),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "    if args['augment']:\n",
    "        transform_cifar_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            cifar_normalize\n",
    "        ])\n",
    "        \n",
    "        transform_mnist_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            mnist_normalize\n",
    "        ])\n",
    "    else:\n",
    "        transform_cifar_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            cifar_normalize\n",
    "        ])\n",
    "        \n",
    "        transform_mnist_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            mnist_normalize\n",
    "        ])\n",
    "\n",
    "    transform_cifar_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        cifar_normalize\n",
    "    ])\n",
    "    \n",
    "    transform_mnist_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        mnist_normalize\n",
    "    ])\n",
    "\n",
    "    if args['data'] == 'cifar10':\n",
    "        trainset = datasets.CIFAR10(root='.Dataset/CIFAR10',\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transform_cifar_train)\n",
    "        testset = datasets.CIFAR10(root='.Dataset/CIFAR10',\n",
    "                                   train=False,\n",
    "                                   download=True,\n",
    "                                   transform=transform_cifar_test)\n",
    "        num_class = 10\n",
    "\n",
    "    if args['data'] == 'mnist':\n",
    "        trainset = datasets.MNIST(root='.Dataset/MNIST', \n",
    "                                  train=True, \n",
    "                                  download=True, \n",
    "                                  transform=transform_mnist_train)\n",
    "\n",
    "        testset = datasets.MNIST(root='.Dataset/MNIST',\n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transform_mnist_test)\n",
    "        \n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global best_acc\n",
    "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "    if args['seed'] != 0:\n",
    "        torch.manual_seed(args['seed'])\n",
    "        np.random.seed(args['seed'])\n",
    "\n",
    "    print('==> Number of lambdas: %g' % args['klam'])\n",
    "\n",
    "    trainset, testset = prepare_data()\n",
    "    trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                              batch_size=len(trainset),\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=8)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(testset,\n",
    "                                             batch_size=args['batch_size'],\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=8)\n",
    "\n",
    "    ## --------------- Create the model --------------- ##\n",
    "    if args['resume']:\n",
    "        # Load checkpoint.\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        assert os.path.isdir(\n",
    "            'checkpoint'), 'Error: no checkpoint directory found!'\n",
    "        \n",
    "        ckptname = os.path.join(\n",
    "        './checkpoint/', f'{args[\"model\"]}_{args[\"data\"]}_{args[\"mode\"]}_{args[\"klam\"]}_{args[\"name\"]}_0.t7')\n",
    "        \n",
    "        checkpoint = torch.load(ckptname)\n",
    "        net = checkpoint['net']\n",
    "        best_acc = checkpoint['acc']\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        rng_state = checkpoint['rng_state']\n",
    "        torch.set_rng_state(rng_state)\n",
    "        \n",
    "        net.cuda()\n",
    "        cudnn.benchmark = True\n",
    "        print('==> Using CUDA..')\n",
    "    else:\n",
    "        print('==> Building model..')\n",
    "        net = models.__dict__[args['model']](num_classes=args['nclass'])\n",
    "\n",
    "    if not os.path.isdir('results'):\n",
    "        os.mkdir('results')\n",
    "    logname = f'results/log_{args[\"model\"]}_{args[\"data\"]}_{args[\"mode\"]}_{args[\"klam\"]}_{args[\"name\"]}_{args[\"trial\"]}.csv'\n",
    "\n",
    "    if use_cuda and not args['resume']:\n",
    "        net.cuda()\n",
    "        net = torch.nn.DataParallel(net)\n",
    "        cudnn.benchmark = True\n",
    "        print('==> Using CUDA..')\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(),\n",
    "                          lr=args[\"lr\"],\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=args[\"decay\"])\n",
    "    \n",
    "    scheduler = MultiStepLR(optimizer, milestones=[100,150], gamma=0.1)\n",
    "\n",
    "    ## --------------- Train and Eval --------------- ##\n",
    "    if not os.path.exists(logname):\n",
    "        with open(logname, 'w') as logfile:\n",
    "            logwriter = csv.writer(logfile, delimiter='\\t')\n",
    "            logwriter.writerow([\n",
    "                'Epoch', 'Train loss', 'Test loss',\n",
    "                'Test acc'\n",
    "            ])\n",
    "\n",
    "    for epoch in range(start_epoch, args['epoch']):\n",
    "        mix_inputs_all, mix_targets_all, original_input, original_label, lams = generate_sample(trainloader)\n",
    "        \n",
    "        if args['mode'] == 'normal':\n",
    "            train_loss, _ = train(\n",
    "                net, optimizer, original_input, original_label, lams, epoch)\n",
    "        else:\n",
    "            train_loss, _ = train(\n",
    "                net, optimizer, mix_inputs_all, mix_targets_all, lams, epoch)\n",
    "        \n",
    "        test_loss, test_acc1, = test(\n",
    "            net, optimizer, testloader, epoch, start_epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "        net.cuda()\n",
    "        with open(logname, 'a') as logfile:\n",
    "            logwriter = csv.writer(logfile, delimiter='\\t')\n",
    "            logwriter.writerow(\n",
    "                [epoch, train_loss, test_loss, test_acc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Number of lambdas: 4\n",
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Resuming from checkpoint..\n",
      "==> Using CUDA..\n",
      "\n",
      "Epoch: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a86622d1354b28ad2bb07dc80e227e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c207176dde94684994a59ddad32fdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving..\n",
      "\n",
      "Epoch: 101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753106669b5d40fbb41731241fb79f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30e20b55335431e9107503f1416feee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87a131dbcf347bba2c158a27063f1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad2f8c17e724ecfa2c142638d3ede79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving..\n",
      "\n",
      "Epoch: 103\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e4e7aea1ec4181922f9d4dd3832b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fff2c5b4c54cde9faaf5cd4f18af7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving..\n",
      "\n",
      "Epoch: 104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530ef29f9dcd4813a8323e6b8410ccd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af17b01cfd0341b7a92737887cc64463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481d243110ca4d4e9226106ceabd096b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b63c0ec86bb40e1904f24319e9e6663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2963bebdfb084575b8cbda3d02c71266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1adf25ac6f4423bc7cfadfe4081e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0159cf5050e245fbaaa11c0dba927928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5338fc71da854f0bbfb117fedc425777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad08a7bc6a184d028c8d32ede58e1b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e96dc5a4fc44fb8b68f01988086336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving..\n",
      "\n",
      "Epoch: 109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a027b499fbe14c319ab8c1cb1cd154d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4b89781ebf46b7b19b1c5dce427455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdab3c300192443ba02dcfb5babac761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bc677cec744bad8f241226319124a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving..\n",
      "\n",
      "Epoch: 111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972b4f2f60814b91bbc285635990abca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args['resume'] = True\n",
    "args['trial'] = 0\n",
    "args['epoch'] = 200\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
